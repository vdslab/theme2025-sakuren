import os
import glob
import unicodedata
import re
import numpy as np
import ctypes
ctypes.cdll.LoadLibrary(r"C:\Program Files\MeCab\bin\libmecab.dll")
import MeCab
import ipadic
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from PIL import Image
import json

# MeCabã®è¨­å®š
mecab = MeCab.Tagger(ipadic.MECAB_ARGS)

# å½¢æ…‹ç´ è§£æï¼‹å‰å‡¦ç†é–¢æ•°
def mecab_tokenizer(text):
    replaced_text = unicodedata.normalize("NFKC", text)
    replaced_text = replaced_text.upper()
    replaced_text = re.sub(r'[ã€ã€‘ ()ï¼ˆï¼‰ã€ã€ã€€ã€Œã€]', '', replaced_text)
    replaced_text = re.sub(r'[[ï¼»ï¼½]]', ' ', replaced_text)
    replaced_text = re.sub(r'[@ï¼ ]\w+', '', replaced_text)
    replaced_text = re.sub(r'\d+\.\d+', '', replaced_text)

    parsed_lines = mecab.parse(replaced_text).split("\n")[:-2]
    surfaces = [l.split("\t")[0] for l in parsed_lines]
    pos = [l.split("\t")[1].split(",")[0] for l in parsed_lines]
    target_pos = ["åè©"]
    token_list = [t for t, p in zip(surfaces, pos) if p in target_pos]

    return ' '.join(token_list)

# éƒ½é“åºœçœŒãƒªã‚¹ãƒˆ
search_words = [
    "æ„›çŸ¥çœŒ", "ç§‹ç”°çœŒ", "é’æ£®çœŒ", "åƒè‘‰çœŒ", "æ„›åª›çœŒ", "ç¦äº•çœŒ", "ç¦å²¡çœŒ", "ç¦å³¶çœŒ", "å²é˜œçœŒ", "ç¾¤é¦¬çœŒ", "åºƒå³¶çœŒ", "åŒ—æµ·é“", "å…µåº«çœŒ",
    "èŒ¨åŸçœŒ", "çŸ³å·çœŒ", "å²©æ‰‹çœŒ", "é¦™å·çœŒ", "é¹¿å…å³¶çœŒ", "ç¥å¥ˆå·çœŒ", "é«˜çŸ¥çœŒ", "ç†Šæœ¬çœŒ", "äº¬éƒ½åºœ", "ä¸‰é‡çœŒ", "å®®åŸçœŒ", "å®®å´çœŒ",
    "é•·é‡çœŒ", "é•·å´çœŒ", "å¥ˆè‰¯çœŒ", "æ–°æ½ŸçœŒ", "å¤§åˆ†çœŒ", "å²¡å±±çœŒ", "æ²–ç¸„çœŒ", "å¤§é˜ªåºœ", "ä½è³€çœŒ", "åŸ¼ç‰çœŒ", "æ»‹è³€çœŒ", "å³¶æ ¹çœŒ",
    "é™å²¡çœŒ", "æ ƒæœ¨çœŒ", "å¾³å³¶çœŒ", "æ±äº¬éƒ½", "é³¥å–çœŒ", "å¯Œå±±çœŒ", "å’Œæ­Œå±±çœŒ", "å±±å½¢çœŒ", "å±±å£çœŒ", "å±±æ¢¨çœŒ"
]
search_words_roma = [
    "aichi", "akita", "aomori", "chiba", "ehime", "fukui", "fukuoka", "fukushima", "gifu", "gunma", "hiroshima", "hokkaido", "hyogo",
    "ibaraki", "ishikawa", "iwate", "kagawa", "kagoshima", "kanagawa", "kochi", "kumamoto", "kyoto", "mie", "miyagi", "miyazaki",
    "nagano", "nagasaki", "nara", "niigata", "oita", "okayama", "okinawa", "osaka", "saga", "saitama", "shiga", "shimane",
    "shizuoka", "tochigi", "tokushima", "tokyo", "tottori", "toyama", "wakayama", "yamagata", "yamaguchi", "yamanashi"
]

# å…¨éƒ½é“åºœçœŒãƒ«ãƒ¼ãƒ—
for i in range(len(search_words)):
    search_word = search_words[i]
    search_word_roma = search_words_roma[i]

    txt_dir = f'./create_wordcloud/tabelog_results/{search_word_roma}'

    documents = []
    for filepath in glob.glob(os.path.join(txt_dir, '*.txt')):
        with open(filepath, encoding='utf-8') as f:
            text = f.read()
            tokenized = mecab_tokenizer(text)
            documents.append(tokenized)

    print(f"èª­ã¿è¾¼ã‚“ã ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(documents)}")
    print(search_word, search_word_roma)

    # TF-IDF è¨ˆç®—
    vectorizer = TfidfVectorizer(max_features=200)
    X = vectorizer.fit_transform(documents)
    words = vectorizer.get_feature_names_out()
    scores = np.asarray(X.mean(axis=0)).ravel()
    word_scores = dict(sorted(zip(words, scores), key=lambda x: x[1], reverse=True))

    # ãƒã‚¹ã‚¯ç”»åƒèª­ã¿è¾¼ã¿
    mask_path = f'./prefecture_layer/{search_word}.png'
    mask_image = Image.open(mask_path).convert("L")
    mask_array = np.array(mask_image)

    # ãƒã‚¹ã‚¯ã®é»’ã„éƒ¨åˆ†ï¼ˆå½¢çŠ¶ï¼‰ã‹ã‚‰æç”»ç¯„å›²ã‚’ç®—å‡º
    mask_indices = np.where(mask_array < 128)
    if mask_indices[0].size == 0 or mask_indices[1].size == 0:
        raise ValueError(f"ãƒã‚¹ã‚¯ç”»åƒã«æœ‰åŠ¹ãªé ˜åŸŸãŒã‚ã‚Šã¾ã›ã‚“: {search_word}")

    min_y_offset = int(np.min(mask_indices[0]))
    max_y_offset = int(np.max(mask_indices[0]))
    min_x_offset = int(np.min(mask_indices[1]))
    max_x_offset = int(np.max(mask_indices[1]))

    # WordCloud æç”»
    font_path = "C:/Windows/Fonts/YuGothR.ttc"
    wordcloud = WordCloud(
        background_color="white",
        width=mask_array.shape[1],
        height=mask_array.shape[0],
        font_path=font_path,
        colormap="coolwarm",
        max_words=200,
        mask=mask_array,
        stopwords=set([
            "è®ƒå²", "å±…é…’å±‹", "åº—", "å††", "å‘³", "æ–™ç†", "ã•ã‚“", "ãƒ©ãƒ¼ãƒ¡ãƒ³", "è‚‰", "ãƒ©ãƒ³ãƒ", "æœ€é«˜", "ãã°", "éºº",
            "é›°å›²æ°—", "ä¸¼", "å®šé£Ÿ", "ãƒ¡ãƒ‹ãƒ¥ãƒ¼", "æº€è¶³", "æ³¨æ–‡", "äºº", "è•éº¦", "æ„Ÿã˜", "åº—å“¡", "æ™®é€š", "ã‚»ãƒƒãƒˆ",
            "2", "æ™‚", "é…’", "æ–¹", "åˆ©ç”¨", "ã†ã©ã‚“", "å€¤æ®µ", "ã”é£¯", "çš„", "æ™‚é–“", "ã‚«ãƒ¬ãƒ¼", "ã‚¹ãƒ¼ãƒ—", "ãƒœãƒªãƒ¥ãƒ¼ãƒ ",
            "é‡", "ä¸­", "å±‹", "ã“ã¨", "è¨ªå•", "1", "ã‚³ãƒ¼ã‚¹", "æ”¾é¡Œ", "åº—å†…", "ç‰›", "ä¸€", "åˆºèº«", "ãƒ¼", "æ¥å®¢",
            "ã“ã“", "ã©ã‚Œ", "æ—¥", "å¥½ã", "ç„¼ã", "å‘³å™Œ", "é‡èœ", "ç¨®é¡", "ãƒ‘", "å¤©ã·ã‚‰", "ä½•", "æ„Ÿ", "äºˆç´„",
            "ã‚«ãƒ„", "ã‚³ã‚¹", "ã‚ˆã†", "é£Ÿäº‹", "æ®‹å¿µ", "æšã’", "å¯¾å¿œ", "ç›®", "é¤ƒå­", "å¯¿å¸", "3", "æ°—", "è±š",
            "å€‹å®¤", "ãã†", "å¸­", "å¡©", "å‰", "è±Šå¯Œ", "ã‚‚ã®", "é­š", "å”", "ãƒãƒ£ãƒ¼ã‚·ãƒ¥ãƒ¼", "ãŠã™ã™ã‚", "ãƒ‘ãƒ³",
            "é§…", "ä»Šæ—¥", "é†¤æ²¹", "ä¸­è¯", "æä¾›", "ç¬‘", "ã“ã‚Œ", "ä¸å¯§", "ã‚µãƒ¼ãƒ“ã‚¹", "ä»Šå›", "ã‚³ã‚¹ãƒ‘", "ã‚µãƒ©ãƒ€"
        ])
    ).generate_from_frequencies(word_scores)

    # JSONãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
    word_layout_data = {"name": search_word, "data": []}

    for (word, font_size, position, orientation, color) in wordcloud.layout_:
        abs_x = float(position[1])
        abs_y = float(position[0])
        rel_x = abs_x - min_x_offset
        rel_y = abs_y - min_y_offset
        norm_x = rel_x / (max_x_offset - min_x_offset)
        norm_y = rel_y / (max_y_offset - min_y_offset)

        word_info = {
            "word": word[0],
            "tfidf_score": word_scores.get(word[0], 0),
            "font_size": font_size,
            "print_area_x": [min_x_offset, max_x_offset],
            "print_area_y": [min_y_offset, max_y_offset],
            "x": round(rel_x, 2),
            "y": round(rel_y, 2),
            "norm_x": round(norm_x, 6),
            "norm_y": round(norm_y, 6),
            "orientation": orientation,
            "color": color
        }
        word_layout_data["data"].append(word_info)

    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜ä¿å­˜
    json_path = "wordcloud_layout.json"
    if os.path.exists(json_path):
        with open(json_path, "r", encoding="utf-8") as f:
            existing_data = json.load(f)
    else:
        existing_data = []

    existing_data.append(word_layout_data)

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(existing_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… {search_word} ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

print("ğŸ‰ å…¨éƒ½é“åºœçœŒã®ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆç”Ÿæˆå®Œäº†")
