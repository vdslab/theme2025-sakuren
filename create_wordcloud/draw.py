import os
import glob
import unicodedata
import re
import numpy as np
import ctypes
ctypes.cdll.LoadLibrary(r"C:\Program Files\MeCab\bin\libmecab.dll")
import MeCab
import ipadic
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from PIL import Image
import json

# MeCabã®è¨­å®š
mecab = MeCab.Tagger(ipadic.MECAB_ARGS)

# å½¢æ…‹ç´ è§£æï¼‹å‰å‡¦ç†é–¢æ•°
def mecab_tokenizer(text):
    replaced_text = unicodedata.normalize("NFKC", text)
    replaced_text = replaced_text.upper()
    replaced_text = re.sub(r'[ã€ã€‘ ()ï¼ˆï¼‰ã€ã€ã€€ã€Œã€]', '', replaced_text)
    replaced_text = re.sub(r'[[ï¼»ï¼½]]', ' ', replaced_text)
    replaced_text = re.sub(r'[@ï¼ ]\w+', '', replaced_text)
    replaced_text = re.sub(r'\d+\.\d+', '', replaced_text)

    parsed_lines = mecab.parse(replaced_text).split("\n")[:-2]
    surfaces = [l.split("\t")[0] for l in parsed_lines]
    pos = [l.split("\t")[1].split(",")[0] for l in parsed_lines]
    target_pos = ["åè©"]
    token_list = [t for t, p in zip(surfaces, pos) if p in target_pos]

    return ' '.join(token_list)

prefectures = [
    "åŒ—æµ·é“", "é’æ£®", "å²©æ‰‹", "å®®åŸ", "ç§‹ç”°", "å±±å½¢", "ç¦å³¶",
    "èŒ¨åŸ", "æ ƒæœ¨", "ç¾¤é¦¬", "åŸ¼ç‰", "åƒè‘‰", "æ±äº¬", "ç¥å¥ˆå·",
    "æ–°æ½Ÿ", "å¯Œå±±", "çŸ³å·", "ç¦äº•", "å±±æ¢¨", "é•·é‡",
    "å²é˜œ", "é™å²¡", "æ„›çŸ¥", "ä¸‰é‡",
    "æ»‹è³€", "äº¬éƒ½", "å¤§é˜ª", "å…µåº«", "å¥ˆè‰¯", "å’Œæ­Œå±±",
    "é³¥å–", "å³¶æ ¹", "å²¡å±±", "åºƒå³¶", "å±±å£",
    "å¾³å³¶", "é¦™å·", "æ„›åª›", "é«˜çŸ¥",
    "ç¦å²¡", "ä½è³€", "é•·å´", "ç†Šæœ¬", "å¤§åˆ†", "å®®å´", "é¹¿å…å³¶", "æ²–ç¸„",
]
# stopwords å®šç¾©
stopwords = set( ["åº—","å††","å‘³","æ–™ç†","ã•ã‚“","ãƒ©ãƒ¼ãƒ¡ãƒ³","è‚‰","ãƒ©ãƒ³ãƒ","æœ€é«˜","ãã°","éºº","é›°å›²æ°—","ä¸¼","å®šé£Ÿ","ãƒ¡ãƒ‹ãƒ¥ãƒ¼","æº€è¶³","æ³¨æ–‡","äºº","è•éº¦","æ„Ÿã˜","åº—å“¡","æ™®é€š","ã‚»ãƒƒãƒˆ","2","æ™‚","é…’","æ–¹","åˆ©ç”¨","ã†ã©ã‚“","å€¤æ®µ","ã”é£¯","çš„","æ™‚é–“","ã‚«ãƒ¬ãƒ¼","ã‚¹ãƒ¼ãƒ—","ãƒœãƒªãƒ¥ãƒ¼ãƒ ","é‡","ä¸­","å±‹","ã“ã¨","è¨ªå•","1","ã‚³ãƒ¼ã‚¹","æ”¾é¡Œ","åº—å†…","ç‰›","ä¸€","åˆºèº«","ãƒ¼","æ¥å®¢","ã“ã“","ã©ã‚Œ","æ—¥","å¥½ã","ç„¼ã","å‘³å™Œ","é‡èœ","ç¨®é¡","ãƒ‘","å¤©ã·ã‚‰","ä½•","æ„Ÿ","äºˆç´„","ã‚«ãƒ„","ã‚³ã‚¹","ã‚ˆã†","é£Ÿäº‹","æ®‹å¿µ","æšã’","å¯¾å¿œ","ç›®","é¤ƒå­","å¯¿å¸","3","æ°—","è±š","å€‹å®¤","ãã†","å¸­","å¡©","å‰","è±Šå¯Œ","ã‚‚ã®","é­š","å”","ãƒãƒ£ãƒ¼ã‚·ãƒ¥ãƒ¼","ãŠã™ã™ã‚","ãƒ‘ãƒ³","é§…","ä»Šæ—¥","é†¤æ²¹","ä¸­è¯","æä¾›","ç¬‘","ã“ã‚Œ","ä¸å¯§","ã‚µãƒ¼ãƒ“ã‚¹","ä»Šå›","ã‚³ã‚¹ãƒ‘","ã‚µãƒ©ãƒ€","æ—¥æœ¬","æ¸©æ³‰", "ãŠæ˜¼", "ã”ã¡ãã†ã•ã¾", "éš å²", "æ¥åº—", "è¿‘æ±Ÿ", "çµç¶æ¹–", "è³¼å…¥", "ç¶ºéº—", "ã‚´ãƒ«ãƒ•", "ä¼šæ´¥", "ç™½æ²³", "é™å®š", "ä»•äº‹", "æ–°é®®","ãŠè…¹", "æ¥åº—", "ä¹…ã—ã¶ã‚Š", "å°æ¹¾", "ã„ã£ã±ã„", "ã”ã¡ãã†ã•ã¾", "é£Ÿå ‚", "è³¼å…¥", "å®¶æ—", "çµ¶å“", "ã‚ªãƒ¼ãƒ€ãƒ¼", "è¶Šå‰", "é§è»Š", "300", "éƒ¡å±±", "é£›é¨¨", "500", "ã¿ãŸã„", "å¥½ã¿", "100", "äººæ°—", "ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³", "æ·¡è·¯å³¶", "ç›´å³¶", "ä¸‰ç›†", "å¥„ç¾", "å¤©è‰", "ä¼Šå‹¢", "ä¿¡å·", "è»½äº•æ²¢", "å¤§å¤‰", "å¹³æ—¥", "äº”å³¶", "ä½ä¸–ä¿", "å³¶åŸ", "æ›¿ç‰", "ç„¡æ–™", "ä¸­æ´¥", "åˆ¥åºœ", "ã‚¹ã‚¿ãƒƒãƒ•", "å®‰å®š", "ä¸€ä¸€", "ä½é‡", "ä¼Šè±†", "é˜¿æ³¢", "é³´é–€", "ç´ æ", "ãƒªãƒ¼ã‚ºãƒŠãƒ–ãƒ«", "è¦ªåˆ‡", "ã‚ªã‚¹ã‚¹ãƒ¡", "éŸ“å›½", "ä¾¡æ ¼", "å±…å¿ƒåœ°", "å…¨éƒ¨", "å¤§å±±", "æ°·è¦‹", ] + prefectures)

# ç”»åƒä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
image_output_dir = "./wordcloud_images"
os.makedirs(image_output_dir, exist_ok=True)
# éƒ½é“åºœçœŒãƒªã‚¹ãƒˆ
search_words = [
    "æ„›çŸ¥çœŒ", "ç§‹ç”°çœŒ", "é’æ£®çœŒ", "åƒè‘‰çœŒ", "æ„›åª›çœŒ", "ç¦äº•çœŒ", "ç¦å²¡çœŒ", "ç¦å³¶çœŒ", "å²é˜œçœŒ", "ç¾¤é¦¬çœŒ", "åºƒå³¶çœŒ", "åŒ—æµ·é“", "å…µåº«çœŒ",
    "èŒ¨åŸçœŒ", "çŸ³å·çœŒ", "å²©æ‰‹çœŒ", "é¦™å·çœŒ", "é¹¿å…å³¶çœŒ", "ç¥å¥ˆå·çœŒ", "é«˜çŸ¥çœŒ", "ç†Šæœ¬çœŒ", "äº¬éƒ½åºœ", "ä¸‰é‡çœŒ", "å®®åŸçœŒ", "å®®å´çœŒ",
    "é•·é‡çœŒ", "é•·å´çœŒ", "å¥ˆè‰¯çœŒ", "æ–°æ½ŸçœŒ", "å¤§åˆ†çœŒ", "å²¡å±±çœŒ", "æ²–ç¸„çœŒ", "å¤§é˜ªåºœ", "ä½è³€çœŒ", "åŸ¼ç‰çœŒ", "æ»‹è³€çœŒ", "å³¶æ ¹çœŒ",
    "é™å²¡çœŒ", "æ ƒæœ¨çœŒ", "å¾³å³¶çœŒ", "æ±äº¬éƒ½", "é³¥å–çœŒ", "å¯Œå±±çœŒ", "å’Œæ­Œå±±çœŒ", "å±±å½¢çœŒ", "å±±å£çœŒ", "å±±æ¢¨çœŒ"
]
search_words_roma = [
    "aichi", "akita", "aomori", "chiba", "ehime", "fukui", "fukuoka", "fukushima", "gifu", "gunma", "hiroshima", "hokkaido", "hyogo",
    "ibaraki", "ishikawa", "iwate", "kagawa", "kagoshima", "kanagawa", "kochi", "kumamoto", "kyoto", "mie", "miyagi", "miyazaki",
    "nagano", "nagasaki", "nara", "niigata", "oita", "okayama", "okinawa", "osaka", "saga", "saitama", "shiga", "shimane",
    "shizuoka", "tochigi", "tokushima", "tokyo", "tottori", "toyama", "wakayama", "yamagata", "yamaguchi", "yamanashi"
]

# å…¨éƒ½é“åºœçœŒãƒ«ãƒ¼ãƒ—
for i in range(len(search_words)):
    search_word = search_words[i]
    search_word_roma = search_words_roma[i]

    txt_dir = f'./create_wordcloud/tabelog_results/{search_word_roma}'

    documents = []
    for filepath in glob.glob(os.path.join(txt_dir, '*.txt')):
        with open(filepath, encoding='utf-8') as f:
            text = f.read()
            tokenized = mecab_tokenizer(text)
            documents.append(tokenized)

    print(f"èª­ã¿è¾¼ã‚“ã ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(documents)}")
    print(search_word, search_word_roma)

    # TF-IDF è¨ˆç®—ï¼ˆ+ stopwords é™¤å»ï¼‰
    vectorizer = TfidfVectorizer(max_features=200)
    X = vectorizer.fit_transform(documents)
    words = vectorizer.get_feature_names_out()
    scores = np.asarray(X.mean(axis=0)).ravel()
    word_scores_raw = dict(zip(words, scores))

    word_scores = {
        word: score for word, score in word_scores_raw.items()
        if word not in stopwords and not word.isdigit()
    }

    # ãƒã‚¹ã‚¯ç”»åƒèª­ã¿è¾¼ã¿
    mask_path = f'./prefecture_layer/{search_word}/{search_word}.png'
    mask_image = Image.open(mask_path).convert("L")
    mask_array = np.array(mask_image)

    mask_indices = np.where(mask_array < 128)
    if mask_indices[0].size == 0 or mask_indices[1].size == 0:
        raise ValueError(f"ãƒã‚¹ã‚¯ç”»åƒã«æœ‰åŠ¹ãªé ˜åŸŸãŒã‚ã‚Šã¾ã›ã‚“: {search_word}")

    min_y_offset = int(np.min(mask_indices[0]))
    max_y_offset = int(np.max(mask_indices[0]))
    min_x_offset = int(np.min(mask_indices[1]))
    max_x_offset = int(np.max(mask_indices[1]))

    # WordCloud æç”»
    font_path = "C:/Windows/Fonts/YuGothR.ttc"
    wordcloud = WordCloud(
        background_color="white",
        width=mask_array.shape[1],
        height=mask_array.shape[0],
        font_path=font_path,
        colormap="coolwarm",
        max_words=200,
        mask=mask_array
    ).generate_from_frequencies(word_scores)
    
    output_img_path = os.path.join(image_output_dir, f"{search_word}.png")
    wordcloud.to_file(output_img_path)
    print(f"ğŸ–¼ï¸ {search_word} ã®ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒã‚’ä¿å­˜: {output_img_path}")


    # JSONãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
    word_layout_data = {"name": search_word, "data": []}

    for (word, font_size, position, orientation, color) in wordcloud.layout_:
        abs_x = float(position[1])
        abs_y = float(position[0])
        rel_x = abs_x - min_x_offset
        rel_y = abs_y - min_y_offset
        norm_x = rel_x / (max_x_offset - min_x_offset)
        norm_y = rel_y / (max_y_offset - min_y_offset)

        word_info = {
            "word": word[0],
            "tfidf_score": word_scores.get(word[0], 0),
            "font_size": font_size,
            "print_area_x": [min_x_offset, max_x_offset],
            "print_area_y": [min_y_offset, max_y_offset],
            "x": round(rel_x, 2),
            "y": round(rel_y, 2),
            "norm_x": round(norm_x, 6),
            "norm_y": round(norm_y, 6),
            "orientation": orientation,
            "color": color
        }
        word_layout_data["data"].append(word_info)

    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜ä¿å­˜
    json_path = "wordcloud_layout.json"
    if os.path.exists(json_path):
        with open(json_path, "r", encoding="utf-8") as f:
            existing_data = json.load(f)
    else:
        existing_data = []

    existing_data.append(word_layout_data)

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(existing_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… {search_word} ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

print("ğŸ‰ å…¨éƒ½é“åºœçœŒã®ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆç”Ÿæˆå®Œäº†")
